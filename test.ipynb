{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import hamming_loss, accuracy_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance, ClassifierChain, LabelPowerset\n",
    "from skmultilearn.adapt import MLkNN\n",
    "from skmultilearn.dataset import load_dataset\n",
    "import pulp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmo(X, y):\n",
    "    selected_samples = []\n",
    "    N, num_features = X.shape\n",
    "    _, M = y.shape\n",
    "\n",
    "    # Current label counts\n",
    "    current_label_counts = np.sum(y, axis=0)\n",
    "    print(\"Label counts before oversampling:\\n\", current_label_counts)\n",
    "\n",
    "    # Target label counts\n",
    "    T = np.max(current_label_counts)\n",
    "    samples_needed_per_label = T - current_label_counts\n",
    "\n",
    "    # Initialize resampled datasets\n",
    "    X_resampled = X.copy()\n",
    "    y_resampled = y.copy()\n",
    "\n",
    "    # Initialize sample costs\n",
    "    sample_costs = np.zeros(N, dtype=int)\n",
    "\n",
    "    # Keep track of how many times each sample has been selected\n",
    "    sample_selection_counts = np.zeros(N, dtype=int)\n",
    "\n",
    "    # While there are labels that need more samples\n",
    "    while np.any(samples_needed_per_label > 0):\n",
    "        # List to hold candidate samples and their scores\n",
    "        candidates = []\n",
    "\n",
    "        for i in range(N):\n",
    "            sample_labels = y[i]\n",
    "            # Potential label counts if we add this sample\n",
    "            potential_counts = current_label_counts + sample_labels\n",
    "            # Check if adding this sample exceeds any target counts\n",
    "            if np.any(potential_counts > T):\n",
    "                continue\n",
    "            # Calculate the contribution\n",
    "            contribution = np.minimum(samples_needed_per_label, sample_labels).sum()\n",
    "            if contribution > 0:\n",
    "                # Calculate the score\n",
    "                score = (sample_costs[i] + 1) / contribution\n",
    "                candidates.append((score, i))\n",
    "\n",
    "            else:\n",
    "                print(np.minimum(samples_needed_per_label, sample_labels))\n",
    "\n",
    "        if not candidates:\n",
    "            print(\"Cannot balance further without exceeding label counts.\")\n",
    "            break\n",
    "\n",
    "        # Select the sample with the lowest score\n",
    "        candidates.sort()\n",
    "        best_score, best_idx = candidates[0]\n",
    "\n",
    "        # Add the sample\n",
    "        X_resampled = np.vstack((X_resampled, X[best_idx:best_idx+1]))\n",
    "        y_resampled = np.vstack((y_resampled, y[best_idx:best_idx+1]))\n",
    "\n",
    "        # Update counts\n",
    "        current_label_counts += y[best_idx]\n",
    "        samples_needed_per_label = T - current_label_counts\n",
    "\n",
    "        # Update sample cost and selection count\n",
    "        sample_selection_counts[best_idx] += 1\n",
    "        sample_costs[best_idx] = sample_selection_counts[best_idx]\n",
    "\n",
    "        selected_samples.append(best_idx)\n",
    "\n",
    "    print(\"Label counts after oversampling:\\n\", current_label_counts)\n",
    "    return X_resampled, y_resampled, selected_samples, sample_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.dataset import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# List of datasets to load\n",
    "datasets = [\n",
    "    'yeast', 'scene', 'emotions', 'bibtex', 'birds', 'cal500',\n",
    "    'corel5k', 'delicious', 'enron', 'genbase', 'mediamill',\n",
    "    'medical', 'tmc2007_500'\n",
    "]\n",
    "\n",
    "# Dictionary to store the datasets\n",
    "data_dict = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    try:\n",
    "        # Load training data\n",
    "        X_train, y_train, _, _ = load_dataset(dataset, 'train')\n",
    "        # Load test data\n",
    "        X_test, y_test, _, _ = load_dataset(dataset, 'test')\n",
    "        \n",
    "        # Store in dictionary\n",
    "        data_dict[dataset] = {\n",
    "            'X_train': X_train,\n",
    "            'y_train': y_train,\n",
    "            'X_test': X_test,\n",
    "            'y_test': y_test\n",
    "        }\n",
    "        \n",
    "        print(f\"Successfully loaded {dataset} dataset.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset} dataset: {e}\")\n",
    "\n",
    "# Example: Accessing the 'yeast' dataset\n",
    "X_train_yeast = data_dict['yeast']['X_train']\n",
    "y_train_yeast = data_dict['yeast']['y_train']\n",
    "X_test_yeast = data_dict['yeast']['X_test']\n",
    "y_test_yeast = data_dict['yeast']['y_test']\n",
    "\n",
    "# Verify the shape of the yeast dataset\n",
    "print(f\"Yeast dataset - X_train shape: {X_train_yeast.shape}, y_train shape: {y_train_yeast.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.asarray(data_dict['yeast']['X_train'].todense())\n",
    "y_train = np.asarray(data_dict['yeast']['y_train'].todense())\n",
    "X_test = np.asarray(data_dict['yeast']['X_test'].todense())\n",
    "y_test = np.asarray(data_dict['yeast']['y_test'].todense())\n",
    "\n",
    "\n",
    "# Normalize features to [0, 1] range\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X_train)\n",
    "\n",
    "# Initialize the Binary Relevance classifier with Random Forest\n",
    "classifier = BinaryRelevance(classifier=RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "# Train the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Hamming Loss: {hamming}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the classifier\n",
    "hamming = hamming_loss(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Hamming Loss: {hamming}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_oversample, y_oversample, _, _ = mmo(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
