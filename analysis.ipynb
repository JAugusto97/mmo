{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import os\n",
    "import pandas as pd\n",
    "import scikit_posthocs as sp\n",
    "from scipy.stats import friedmanchisquare\n",
    "\n",
    "# %%\n",
    "# Load all CSVs into a single DataFrame\n",
    "def load_data(data_path):\n",
    "    dataframes = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(data_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply highlighting for the maximum values in each row\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: red' if v else '' for v in is_max]\n",
    "\n",
    "# Perform the Friedman test and follow up with Nemenyi post-hoc test if significant\n",
    "def perform_friedman_and_nemenyi(merged_df, methods, metric):\n",
    "    results = []\n",
    "    all_ranks = {method: [] for method in methods}  # Dictionary to store ranks across all setups\n",
    "    \n",
    "    for classifier in merged_df[\"Classifier\"].unique():\n",
    "        for dataset in merged_df[\"Dataset\"].unique():\n",
    "            blocks = merged_df['Seed'].unique()\n",
    "            data_list = []\n",
    "            \n",
    "            for method in methods:\n",
    "                method_data = []\n",
    "                for block in blocks:\n",
    "                    val = merged_df[\n",
    "                        (merged_df[\"Classifier\"] == classifier) & \n",
    "                        (merged_df[\"Dataset\"] == dataset) & \n",
    "                        (merged_df[\"Oversampling\"] == method) & \n",
    "                        (merged_df['Seed'] == block)\n",
    "                    ][metric].values\n",
    "                    if len(val) > 0:\n",
    "                        method_data.append(val[0])\n",
    "                    else:\n",
    "                        method_data.append(np.nan)\n",
    "                data_list.append(method_data)\n",
    "            \n",
    "            # Convert to numpy array and remove rows with NaN\n",
    "            data_array = np.array(data_list).T\n",
    "            data_array = data_array[~np.isnan(data_array).any(axis=1)]\n",
    "            \n",
    "            if data_array.shape[0] > 0:\n",
    "                # Perform the Friedman test\n",
    "                stat, p_value = friedmanchisquare(*data_array.T)\n",
    "                \n",
    "                # Calculate ranks for each method in this specific classifier-dataset setup\n",
    "                ranks = np.argsort(np.argsort(-data_array, axis=1), axis=1) + 1\n",
    "                for i, method in enumerate(methods):\n",
    "                    all_ranks[method].extend(ranks[:, i])  # Accumulate ranks across all setups\n",
    "                \n",
    "                # If the Friedman test is significant, perform Nemenyi post hoc test\n",
    "                if p_value < 0.05:\n",
    "                    data = pd.DataFrame(data_array, columns=methods)\n",
    "                    nemenyi_results = sp.posthoc_nemenyi_friedman(data)\n",
    "                    \n",
    "                    # Use median to find the best method\n",
    "                    median_scores = data.median()\n",
    "                    best_method = median_scores.idxmax()\n",
    "                    \n",
    "                    # Check if best method is significantly better than all others\n",
    "                    is_best_method_significant = all(\n",
    "                        nemenyi_results.loc[best_method, method] < 0.05\n",
    "                        for method in methods if method != best_method\n",
    "                    )\n",
    "                    \n",
    "                    results.append({\n",
    "                        \"Classifier\": classifier,\n",
    "                        \"Dataset\": dataset,\n",
    "                        \"Best Method\": best_method if is_best_method_significant else None,\n",
    "                        \"p-value\": p_value\n",
    "                    })\n",
    "                else:\n",
    "                    results.append({\n",
    "                        \"Classifier\": classifier,\n",
    "                        \"Dataset\": dataset,\n",
    "                        \"Best Method\": None,\n",
    "                        \"p-value\": p_value\n",
    "                    })\n",
    "    \n",
    "    # Compute median rank across all ranks for each method\n",
    "    median_rank_df = pd.DataFrame({\n",
    "        \"Method\": methods,\n",
    "        \"Median Rank\": [np.mean(all_ranks[method]) for method in methods]\n",
    "    })\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    return result_df, median_rank_df\n",
    "\n",
    "\n",
    "# Create a pivot table for the F1 Macro scores and apply statistical significance\n",
    "def create_styled_pivot(merged_df, friedman_results_df, methods, metric):\n",
    "    f1_macro_pivot = merged_df.groupby([\"Classifier\", \"Dataset\", \"Oversampling\"]).mean()[metric].reset_index()\n",
    "    f1_macro_pivot = f1_macro_pivot.pivot_table(index=[\"Classifier\", \"Dataset\"], columns=\"Oversampling\", values=metric)\n",
    "    f1_macro_pivot = f1_macro_pivot[methods]\n",
    "    \n",
    "    # Create a mask to highlight the maximum values in each row\n",
    "    max_highlight_mask = f1_macro_pivot.apply(lambda row: row == row.max(), axis=1)\n",
    "    styled_f1_macro_pivot = f1_macro_pivot.copy()\n",
    "    \n",
    "    # Mark statistically significant best method based on Friedman and Nemenyi results\n",
    "    for _, row in friedman_results_df.iterrows():\n",
    "        classifier, dataset, best_method = row[\"Classifier\"], row[\"Dataset\"], row[\"Best Method\"]\n",
    "        if best_method:\n",
    "            styled_f1_macro_pivot.loc[(classifier, dataset), best_method] = f\"{f1_macro_pivot.loc[(classifier, dataset), best_method]:.5f}*\"\n",
    "    \n",
    "    # Apply the highlight function to the DataFrame\n",
    "    return styled_f1_macro_pivot.style.apply(lambda s: ['background-color: red' if is_max else '' for is_max in max_highlight_mask.loc[s.name]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = 'datasets'\n",
    "    # methods_combined = [\"none\", \"ml_smote\", \"mmo_smote\"]\n",
    "    methods_combined = [\"none\", \"ml_ros\", \"mmo\", \"ml_smote\", \"mmo_smote\"]\n",
    "    metric = \"F1 Macro\"\n",
    "    \n",
    "    # Load and prepare data\n",
    "    merged_df = load_data(data_path)\n",
    "\n",
    "    # Perform Friedman and Nemenyi post-hoc tests for combined methods\n",
    "    friedman_results_combined, friedman_results_rank = perform_friedman_and_nemenyi(merged_df, methods_combined, metric)\n",
    "\n",
    "    # Create styled pivot table for combined methods\n",
    "    styled_combined_pivot = create_styled_pivot(merged_df, friedman_results_combined, methods_combined, metric)\n",
    "    display(styled_combined_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "friedman_results_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "f1_macro_pivot = merged_df.groupby([\"Classifier\", \"Dataset\", \"Oversampling\"]).mean()[\"Train_Set_Increase\"].reset_index()\n",
    "f1_macro_pivot = f1_macro_pivot.pivot_table(index=[\"Classifier\", \"Dataset\"], columns=\"Oversampling\", values=\"Train_Set_Increase\")\n",
    "f1_macro_pivot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
