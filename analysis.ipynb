{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSVs into a single DataFrame\n",
    "def load_data(data_path):\n",
    "    dataframes = []\n",
    "    for filename in os.listdir(data_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(data_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply highlighting for the maximum values in each row\n",
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: red' if v else '' for v in is_max]\n",
    "\n",
    "# Perform the Wilcoxon test for paired comparisons between methods\n",
    "def perform_wilcoxon_tests(merged_df, comparisons, metric):\n",
    "    wilcoxon_results = []\n",
    "    for classifier in merged_df[\"Classifier\"].unique():\n",
    "        for dataset in merged_df[\"Dataset\"].unique():\n",
    "            for method_1, method_2 in comparisons:\n",
    "                data_1 = merged_df[(merged_df[\"Classifier\"] == classifier) & \n",
    "                                   (merged_df[\"Dataset\"] == dataset) & \n",
    "                                   (merged_df[\"Oversampling\"] == method_1)][metric].values\n",
    "                data_2 = merged_df[(merged_df[\"Classifier\"] == classifier) & \n",
    "                                   (merged_df[\"Dataset\"] == dataset) & \n",
    "                                   (merged_df[\"Oversampling\"] == method_2)][metric].values\n",
    "\n",
    "                if len(data_1) > 0 and len(data_1) == len(data_2):\n",
    "                    stat, p_value = wilcoxon(data_1, data_2)\n",
    "                    wilcoxon_results.append({\n",
    "                        \"Classifier\": classifier,\n",
    "                        \"Dataset\": dataset,\n",
    "                        \"Comparison\": f\"{method_1} vs {method_2}\",\n",
    "                        \"Statistic\": stat,\n",
    "                        \"p-value\": p_value\n",
    "                    })\n",
    "    return pd.DataFrame(wilcoxon_results)\n",
    "\n",
    "# Create a pivot table for the F1 Macro scores and apply statistical significance\n",
    "def create_styled_pivot(merged_df, wilcoxon_results_df, methods, metric):\n",
    "    f1_macro_pivot = merged_df.groupby([\"Classifier\", \"Dataset\", \"Oversampling\"]).mean()[metric].reset_index()\n",
    "    f1_macro_pivot = f1_macro_pivot.pivot_table(index=[\"Classifier\", \"Dataset\"], columns=\"Oversampling\", values=metric)\n",
    "    f1_macro_pivot = f1_macro_pivot[methods]\n",
    "    \n",
    "    # Create a mask to highlight the maximum values in each row\n",
    "    max_highlight_mask = f1_macro_pivot.apply(lambda row: row == row.max(), axis=1)\n",
    "    styled_f1_macro_pivot = f1_macro_pivot.copy()\n",
    "    \n",
    "    # Mark statistically significant results on the highest F1-macro score\n",
    "    for _, row in wilcoxon_results_df.iterrows():\n",
    "        classifier, dataset, comparison, p_value = row[\"Classifier\"], row[\"Dataset\"], row[\"Comparison\"], row[\"p-value\"]\n",
    "        if p_value < 0.05:\n",
    "            method_1, method_2 = comparison.split(\" vs \")\n",
    "            max_score = styled_f1_macro_pivot.loc[(classifier, dataset)].max()\n",
    "            if f1_macro_pivot.loc[(classifier, dataset), method_1] == max_score:\n",
    "                styled_f1_macro_pivot.loc[(classifier, dataset), method_1] = f\"{f1_macro_pivot.loc[(classifier, dataset), method_1]:.5f}*\"\n",
    "            elif f1_macro_pivot.loc[(classifier, dataset), method_2] == max_score:\n",
    "                styled_f1_macro_pivot.loc[(classifier, dataset), method_2] = f\"{f1_macro_pivot.loc[(classifier, dataset), method_2]:.5f}*\"\n",
    "    \n",
    "    # Apply the highlight function to the DataFrame\n",
    "    return styled_f1_macro_pivot.style.apply(lambda s: ['background-color: red' if is_max else '' for is_max in max_highlight_mask.loc[s.name]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    data_path = 'datasets'\n",
    "    methods_smote = [\"none\", \"ml_smote\", \"mmo_smote\"]\n",
    "    methods_ros = [\"none\", \"ml_ros\", \"mmo\"]\n",
    "    metric = \"F1 Macro\"\n",
    "    # Load and prepare data\n",
    "    merged_df = load_data(data_path)\n",
    "\n",
    "    # Perform Wilcoxon tests for SMOTE comparisons\n",
    "    comparisons_smote = [(\"ml_smote\", \"mmo_smote\")]\n",
    "    wilcoxon_results_smote = perform_wilcoxon_tests(merged_df, comparisons_smote, metric)\n",
    "\n",
    "    # Create styled pivot table for SMOTE comparisons\n",
    "    styled_smote_pivot = create_styled_pivot(merged_df, wilcoxon_results_smote, methods_smote, metric)\n",
    "    display(styled_smote_pivot)\n",
    "\n",
    "    # Perform Wilcoxon tests for ROS comparisons\n",
    "    comparisons_ros = [(\"ml_ros\", \"mmo\")]\n",
    "    wilcoxon_results_ros = perform_wilcoxon_tests(merged_df, comparisons_ros, metric)\n",
    "\n",
    "    # Create styled pivot table for ROS comparisons\n",
    "    styled_ros_pivot = create_styled_pivot(merged_df, wilcoxon_results_ros, methods_ros, metric)\n",
    "    display(styled_ros_pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_macro_pivot = merged_df.groupby([\"Classifier\", \"Dataset\", \"Oversampling\"]).mean()[\"Train_Set_Increase\"].reset_index()\n",
    "f1_macro_pivot = f1_macro_pivot.pivot_table(index=[\"Classifier\", \"Dataset\"], columns=\"Oversampling\", values=\"Train_Set_Increase\")\n",
    "f1_macro_pivot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
